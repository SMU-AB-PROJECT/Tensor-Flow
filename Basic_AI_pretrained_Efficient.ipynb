{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Basic_AI_pretrained_Efficient.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "ne1BTMLkFy_R"
      },
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow.keras\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.applications import EfficientNetB3, InceptionV3, ResNet101, ResNet50\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.layers import Input, Dense, Dropout, Flatten\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint, ReduceLROnPlateau, EarlyStopping\n",
        "from tensorflow.keras.experimental import CosineDecay\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "import cv2\n",
        "import numpy as np\n",
        "import os"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nWq4Or_ofQkO"
      },
      "source": [
        "!mkdir dataset"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tK9lFxneLgw1"
      },
      "source": [
        "!unzip /content/drive/MyDrive/data/data.zip -d /content/dataset"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H6uaXxs3g_V3"
      },
      "source": [
        "train_path = '/content/dataset'\n",
        "labels_list = os.listdir(train_path)\n",
        "img_size = 224\n",
        "img_shape = (img_size,img_size,3)\n",
        "batch_size = 32\n",
        "epochs = 10\n",
        "dropout_rate = 0.5\n",
        "num_of_predict = len(labels_list)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gUga8Im2F0W6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0c0c191e-fbaa-4b96-d8cb-3553d65e3ea1"
      },
      "source": [
        "train_datagen = ImageDataGenerator(rescale=1./255,\n",
        "                                  shear_range=0.2,\n",
        "                                  zoom_range=0.2,\n",
        "                                  horizontal_flip=True)\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "    train_path,\n",
        "    target_size=(img_size,img_size),\n",
        "    batch_size=batch_size,\n",
        "    class_mode='categorical'\n",
        ")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 2866 images belonging to 12 classes.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h3V_lUg0hMyu",
        "outputId": "61fbb405-3a70-41aa-9a23-588a9f0bbcc1"
      },
      "source": [
        "efficientnet = EfficientNetB3(weights=\"/content/drive/MyDrive/data/efficientnetb3_notop.h5\",\n",
        "                              include_top=False,\n",
        "                              input_shape=img_shape,\n",
        "                              drop_connect_rate=dropout_rate)\n",
        "inputs = Input(shape=img_shape)\n",
        "efficientnet = efficientnet(inputs)\n",
        "pooling = layers.GlobalAveragePooling2D()(efficientnet)\n",
        "dropout = layers.Dropout(dropout_rate)(pooling)\n",
        "outputs = Dense(num_of_predict, activation=\"softmax\")(dropout)\n",
        "eff_model = Model(inputs=inputs, outputs=outputs)\n",
        "\n",
        "eff_model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_2 (InputLayer)         [(None, 224, 224, 3)]     0         \n",
            "_________________________________________________________________\n",
            "efficientnetb3 (Functional)  (None, 7, 7, 1536)        10783535  \n",
            "_________________________________________________________________\n",
            "global_average_pooling2d (Gl (None, 1536)              0         \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 1536)              0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 12)                18444     \n",
            "=================================================================\n",
            "Total params: 10,801,979\n",
            "Trainable params: 10,714,676\n",
            "Non-trainable params: 87,303\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ait9nczzhOSK"
      },
      "source": [
        "decay_steps = int(round(2866/batch_size))*epochs\n",
        "cosine_decay = CosineDecay(initial_learning_rate=1e-4, decay_steps=decay_steps, alpha=0.3)\n",
        "eff_model.compile(loss=tf.keras.losses.CategoricalCrossentropy(), optimizer=tf.keras.optimizers.Adam(cosine_decay), metrics=[\"accuracy\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OjdKA2lqlLfD",
        "outputId": "4897a07e-a7a9-4308-d7b2-dec97a83334d"
      },
      "source": [
        "history = eff_model.fit(train_generator,\n",
        "                       epochs = 10)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            " 4/90 [>.............................] - ETA: 1:02 - loss: 2.7142 - accuracy: 0.0768"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "90/90 [==============================] - 110s 723ms/step - loss: 2.3284 - accuracy: 0.2179\n",
            "Epoch 2/10\n",
            "90/90 [==============================] - 64s 708ms/step - loss: 1.1714 - accuracy: 0.6766\n",
            "Epoch 3/10\n",
            "90/90 [==============================] - 64s 708ms/step - loss: 0.6676 - accuracy: 0.8202\n",
            "Epoch 4/10\n",
            "90/90 [==============================] - 64s 704ms/step - loss: 0.4118 - accuracy: 0.8798\n",
            "Epoch 5/10\n",
            "90/90 [==============================] - 64s 704ms/step - loss: 0.3475 - accuracy: 0.9023\n",
            "Epoch 6/10\n",
            "90/90 [==============================] - 64s 707ms/step - loss: 0.2696 - accuracy: 0.9312\n",
            "Epoch 7/10\n",
            "90/90 [==============================] - 64s 708ms/step - loss: 0.2472 - accuracy: 0.9324\n",
            "Epoch 8/10\n",
            "90/90 [==============================] - 64s 703ms/step - loss: 0.1974 - accuracy: 0.9502\n",
            "Epoch 9/10\n",
            "90/90 [==============================] - 64s 704ms/step - loss: 0.1486 - accuracy: 0.9594\n",
            "Epoch 10/10\n",
            "90/90 [==============================] - 65s 716ms/step - loss: 0.1705 - accuracy: 0.9485\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NRpuwWhil8cA",
        "outputId": "23b593fd-01e6-45cf-b337-048f4f3bad36"
      },
      "source": [
        "eff_model.evaluate(train_generator)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "21/90 [======>.......................] - ETA: 37s - loss: 0.1723 - accuracy: 0.9524"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "90/90 [==============================] - 49s 525ms/step - loss: 0.1714 - accuracy: 0.9532\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.17139869928359985, 0.9532449245452881]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8KIX_VoGmD2N"
      },
      "source": [
        "eff_model.save('/content/eff_tmp.h5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FJKY7vijmKOJ"
      },
      "source": [
        "test_model = tf.keras.models.load_model('/content/eff_tmp.h5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v-8G6ZX1nIWF",
        "outputId": "aad0dd62-a489-430e-cf4b-135003644212"
      },
      "source": [
        "test_model.evaluate(train_generator)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " 6/90 [=>............................] - ETA: 48s - loss: 0.1508 - accuracy: 0.9740"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "90/90 [==============================] - 50s 533ms/step - loss: 0.1753 - accuracy: 0.9491\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.17527298629283905, 0.949057936668396]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MSWpfiKqlxfv"
      },
      "source": [
        "# eff_model.save('/content/drive/MyDrive/data/eff_final.h5')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}