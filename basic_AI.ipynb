{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "basic_AI.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "jpwsNEeCbowP"
      },
      "source": [
        "# !wget http://data.vision.ee.ethz.ch/cvl/food-101.tar.gz"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TThq2Jxubq2C"
      },
      "source": [
        "# !tar xzvf food-101.tar.gz"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zlks7mJEcvvv"
      },
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow.keras\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.applications import EfficientNetB3, InceptionV3, ResNet101\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.layers import Input, Dense, Dropout, Flatten \n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint, ReduceLROnPlateau, EarlyStopping\n",
        "from tensorflow.keras.experimental import CosineDecay\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qAmW83t1F2iL",
        "outputId": "c3cac7b6-a71f-412e-992e-fce1654e110d"
      },
      "source": [
        "print(tf.__version__)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2.4.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OC7xbyRbixR7",
        "outputId": "f8075c7e-6043-4f7b-89ef-1ff56e6946f6"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IeZlSlZHkVan"
      },
      "source": [
        "# from google.colab import files\n",
        "\n",
        "# files.upload()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CCVKoI3ImTLh"
      },
      "source": [
        "# train_path = '../input/food101/training'\n",
        "# val_path = '../input/food101/validation'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jsRosxKBq9HL"
      },
      "source": [
        "!unzip /content/drive/MyDrive/data/dset.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Tdc8GjKrXYT"
      },
      "source": [
        "train_path = '/content/dset'"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JlHolGcAyuou"
      },
      "source": [
        "# Deep Learning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yoz3I158y2K-"
      },
      "source": [
        "# Convolution neural network (CNN)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_u33gv9CwBc0"
      },
      "source": [
        "# Data Generator"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QsDLHx5tzFt6"
      },
      "source": [
        "img_size = 224\n",
        "img_shape = (img_size,img_size,3)\n",
        "batch_size = 32\n",
        "epochs = 8\n",
        "dropout_rate = 0.5\n",
        "num_of_predict = 4"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PJqs-1ffvFvi"
      },
      "source": [
        "train_datagen = ImageDataGenerator(rescale=1./255,\n",
        "                                  shear_range=0.2,\n",
        "                                  zoom_range=0.2,\n",
        "                                  horizontal_flip=True)\n",
        "test_datagen = ImageDataGenerator(rescale=1./255)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MHCmFXZnvHL8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a7449eb6-d5e9-45cf-a8ca-bd33d9b5a0e9"
      },
      "source": [
        "train_generator = train_datagen.flow_from_directory(\n",
        "    train_path,\n",
        "    target_size=(img_size,img_size),\n",
        "    batch_size=batch_size,\n",
        "    class_mode='categorical'\n",
        ")\n",
        "# val_generator = train_datagen.flow_from_directory(\n",
        "#     val_path,\n",
        "#     target_size=(img_size,img_size),\n",
        "#     batch_size=batch_size,\n",
        "#     class_mode='categorical'\n",
        "# )"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found 423 images belonging to 4 classes.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f-zPLcRowHna"
      },
      "source": [
        "# Base model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GXnGZBXywLG_"
      },
      "source": [
        "## EfficientNet B3  \n",
        "Tan, Mingxing, and Quoc Le. \"Efficientnet: Rethinking model scaling for convolutional neural networks.\" International Conference on Machine Learning. PMLR, 2019."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QPBMb8qsvH3b",
        "outputId": "4be6158d-1049-4279-d399-139890907b35"
      },
      "source": [
        "efficientnet = EfficientNetB3(\n",
        "    include_top=False, weights='imagenet',\n",
        "    input_shape=img_shape, classes = num_of_predict,\n",
        "    classifier_activation='softmax'\n",
        ")\n",
        "\n",
        "inputs = Input(shape=img_shape)\n",
        "efficientnet = efficientnet(inputs)\n",
        "pooling = layers.GlobalAveragePooling2D()(efficientnet)\n",
        "dropout = layers.Dropout(dropout_rate)(pooling)\n",
        "outputs = Dense(num_of_predict, activation=\"softmax\")(dropout)\n",
        "eff_model = Model(inputs=inputs, outputs=outputs)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/keras-applications/efficientnetb3_notop.h5\n",
            "43941888/43941136 [==============================] - 0s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tILl4476wOFN"
      },
      "source": [
        "## InceptionV3  \n",
        "Szegedy, Christian, et al. \"Rethinking the inception architecture for computer vision.\" Proceedings of the IEEE conference on computer vision and pattern recognition. 2016."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VlxYXUPjvJoB",
        "outputId": "a9517265-ef23-405d-ddb0-f1004cb6318d"
      },
      "source": [
        "inception = InceptionV3(\n",
        "    include_top=False, weights='imagenet',\n",
        "    input_shape=img_shape, classes = num_of_predict,\n",
        "    classifier_activation='softmax')\n",
        "\n",
        "inputs = Input(shape=img_shape)\n",
        "inception = inception(inputs)\n",
        "pooling = layers.GlobalAveragePooling2D()(inception)\n",
        "dropout = layers.Dropout(dropout_rate)(pooling)\n",
        "outputs = Dense(num_of_predict, activation=\"softmax\")(dropout)\n",
        "inception_model = Model(inputs=inputs, outputs=outputs)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/inception_v3/inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "87916544/87910968 [==============================] - 0s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GUxFHlIlwQzl"
      },
      "source": [
        "## ResNet101  \n",
        "He, Kaiming, et al. \"Deep residual learning for image recognition.\" Proceedings of the IEEE conference on computer vision and pattern recognition. 2016."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ubv5WuwFvNPt",
        "outputId": "072d1aaa-c036-4230-b752-3fb481880940"
      },
      "source": [
        "ResNet101 = ResNet101(\n",
        "    include_top=False, weights='imagenet',\n",
        "    input_shape=img_shape, classes = num_of_predict,\n",
        "    classifier_activation='softmax'\n",
        ")\n",
        "\n",
        "inputs = Input(shape=img_shape)\n",
        "ResNet101 = ResNet101(inputs)\n",
        "pooling = layers.GlobalAveragePooling2D()(ResNet101)\n",
        "dropout = layers.Dropout(dropout_rate)(pooling)\n",
        "outputs = Dense(num_of_predict, activation=\"softmax\")(dropout)\n",
        "res_model = Model(inputs=inputs, outputs=outputs)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/resnet/resnet101_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "171450368/171446536 [==============================] - 1s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "05SyG278saMP",
        "outputId": "81379e87-d6e8-4310-eb05-aec3e37a1c0f"
      },
      "source": [
        "res_model.summary()"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"model_2\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "input_6 (InputLayer)         [(None, 224, 224, 3)]     0         \n",
            "_________________________________________________________________\n",
            "resnet101 (Functional)       (None, 7, 7, 2048)        42658176  \n",
            "_________________________________________________________________\n",
            "global_average_pooling2d_2 ( (None, 2048)              0         \n",
            "_________________________________________________________________\n",
            "dropout_2 (Dropout)          (None, 2048)              0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 4)                 8196      \n",
            "=================================================================\n",
            "Total params: 42,666,372\n",
            "Trainable params: 42,561,028\n",
            "Non-trainable params: 105,344\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gS5Urfyrwr06"
      },
      "source": [
        "# Train"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ua2dUesyvNCC"
      },
      "source": [
        "decay_steps = int(round(423/batch_size))*epochs\n",
        "cosine_decay = CosineDecay(initial_learning_rate=1e-4, decay_steps=decay_steps, alpha=0.3)\n",
        "\n",
        "callbacks = [ModelCheckpoint(filepath='best_model.h5', monitor='val_loss', save_best_only=True)]\n",
        "\n",
        "eff_model.compile(loss=tf.keras.losses.CategoricalCrossentropy(), optimizer=tf.keras.optimizers.Adam(cosine_decay), metrics=[\"accuracy\"])"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kcmSWLKnztPs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fbccd932-30c2-4f71-e4a5-a877c0d87ff9"
      },
      "source": [
        "history = eff_model.fit(train_generator,\n",
        "                  epochs = epochs)"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            " 8/14 [================>.............] - ETA: 5s - loss: 1.3824 - accuracy: 0.3564"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "14/14 [==============================] - 25s 801ms/step - loss: 1.2763 - accuracy: 0.4299\n",
            "Epoch 2/10\n",
            "14/14 [==============================] - 10s 707ms/step - loss: 0.6881 - accuracy: 0.8096\n",
            "Epoch 3/10\n",
            "14/14 [==============================] - 10s 720ms/step - loss: 0.3599 - accuracy: 0.9139\n",
            "Epoch 4/10\n",
            "14/14 [==============================] - 11s 701ms/step - loss: 0.2798 - accuracy: 0.9346\n",
            "Epoch 5/10\n",
            "14/14 [==============================] - 10s 701ms/step - loss: 0.2127 - accuracy: 0.9541\n",
            "Epoch 6/10\n",
            "14/14 [==============================] - 10s 716ms/step - loss: 0.1464 - accuracy: 0.9720\n",
            "Epoch 7/10\n",
            "14/14 [==============================] - 10s 694ms/step - loss: 0.1314 - accuracy: 0.9870\n",
            "Epoch 8/10\n",
            "14/14 [==============================] - 10s 699ms/step - loss: 0.1306 - accuracy: 0.9663\n",
            "Epoch 9/10\n",
            "14/14 [==============================] - 10s 728ms/step - loss: 0.1031 - accuracy: 0.9860\n",
            "Epoch 10/10\n",
            "14/14 [==============================] - 10s 710ms/step - loss: 0.0925 - accuracy: 0.9913\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TOY26z6jzvze"
      },
      "source": [
        "# Visualization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DqgkOZM-0IRv"
      },
      "source": [
        "accuracy = history.history['accuracy']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0XLD0PXQzzJA"
      },
      "source": [
        "accuracy\n",
        "x = range(len(accuracy))\n",
        "y2 = accuracy\n",
        "plt.plot(x, y2)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BmzPvd-z1N7D"
      },
      "source": [
        "predictions = model.predict(augmented_images)\n",
        "final_prediction = np.argmax(global_predictions)\n",
        "\n",
        "def run_predictions_over_image_list(image_list, folder):\n",
        "    predictions = []\n",
        "    with tqdm(total=len(image_list)) as pbar:\n",
        "        for image_filename in image_list:\n",
        "            pbar.update(1)\n",
        "            predictions.append(predict_and_vote(image_filename, folder))\n",
        "    return predictions\n",
        "\n",
        "validation_df[\"results\"] = run_predictions_over_image_list(validation_df[\"image_id\"], training_folder)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LkSyYJsG0yG8"
      },
      "source": [
        "true_positives = 0\n",
        "prediction_distribution_per_class = {\"0\":{\"0\": 0, \"1\": 0, \"2\":0, \"3\":0, \"4\":0}, # class X class 사이즈 로\n",
        "                                     \"1\":{\"0\": 0, \"1\": 0, \"2\":0, \"3\":0, \"4\":0},\n",
        "                                     \"2\":{\"0\": 0, \"1\": 0, \"2\":0, \"3\":0, \"4\":0},\n",
        "                                     \"3\":{\"0\": 0, \"1\": 0, \"2\":0, \"3\":0, \"4\":0},\n",
        "                                     \"4\":{\"0\": 0, \"1\": 0, \"2\":0, \"3\":0, \"4\":0}}\n",
        "number_of_images = len(validation_df)\n",
        "for idx, pred in validation_df.iterrows():\n",
        "    if int(pred[\"label\"]) == pred.results:\n",
        "        true_positives+=1\n",
        "    prediction_distribution_per_class[str(pred[\"label\"])][str(pred.results)] += 1\n",
        "print(\"accuracy: {}%\".format(true_positives/number_of_images*100))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_KWVchbJ0y4E"
      },
      "source": [
        "prediction_distribution_per_class"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GhhnuvQh01LI"
      },
      "source": [
        "heatmap_df = pd.DataFrame(columns={\"groundtruth\",\"prediction\",\"value\"})\n",
        "for key in prediction_distribution_per_class.keys():\n",
        "    for pred_key in prediction_distribution_per_class[key].keys():\n",
        "        value = prediction_distribution_per_class[key][pred_key]/validation_df.query(\"label==@key\").count()[0]\n",
        "        heatmap_df = heatmap_df.append({\"groundtruth\":key,\"prediction\":pred_key,\"value\":value}, ignore_index=True)   \n",
        "\n",
        "heatmap = heatmap_df.pivot(index='groundtruth', columns='prediction', values='value')\n",
        "sns.heatmap(heatmap,cmap=\"Blues\")"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}